{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e809f467-21c5-41ca-adfb-ba27d2a2eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e01967f-082e-461d-942e-05198b44ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Preparation\n",
    "# Loading the datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fd93d8c-8f43-4f86-83af-d8b1aa163fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture of the model\n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # First convolutional block\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=5)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.15)\n",
    "\n",
    "        # Second convolutional block\n",
    "\n",
    "        self.conv3 = nn.Conv2d(16, 16, kernel_size=5, padding=2)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(16, 16, kernel_size=5)\n",
    "\n",
    "        self.dropout2 = nn.Dropout(0.15)\n",
    "\n",
    "        # Global Average Pooling (GAP) to reduce the number of parameters\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)  # GAP layer to reduce feature map size\n",
    "\n",
    "        # Fully connected layers\n",
    "\n",
    "        self.fc1 = nn.Linear(16, 10)  # Only 16 features after GAP, output: 10 classes\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # First conv block with residual connection\n",
    "\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Second conv block\n",
    "\n",
    "        x = F.relu(self.bn2(self.conv3(x)))\n",
    "\n",
    "        x = F.relu(self.conv4(x))\n",
    "\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Apply GAP (Global Average Pooling)\n",
    "\n",
    "        x = self.gap(x)  # Output will be of shape (batch_size, 16, 1, 1)\n",
    "\n",
    "        # Flatten and apply final fully connected layer\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, 16)\n",
    "        x = self.fc1(x)  # Output 10 classes\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439199ab-1a2e-4b7d-a0ed-cd2234fb1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing\n",
    "\n",
    "# Training Function\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval=100):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss = F.nll_loss(output, target)  # Ensure both have the same batch size\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "\n",
    "\n",
    "# Testing Function\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad(): \n",
    "\n",
    "        for data, target in test_loader:\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "\n",
    "            pred = output.argmax(dim=1, keepdim=True) \n",
    "\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    \n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    \n",
    "\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, '\n",
    "\n",
    "          f'Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8393c58a-0a24-41a6-9326-6114ed05659c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device Configuration\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "102a62fe-cba3-46b1-83de-450f2f175fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ERA_V3\\cpu_env\\myenv_cpu\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and optimizer\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-5)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "591eeeac-12be-43bb-9018-a5f6a327a92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 28, 28]             416\n",
      "       BatchNorm2d-2           [-1, 16, 28, 28]              32\n",
      "            Conv2d-3           [-1, 16, 24, 24]           6,416\n",
      "         MaxPool2d-4           [-1, 16, 12, 12]               0\n",
      "           Dropout-5           [-1, 16, 12, 12]               0\n",
      "            Conv2d-6           [-1, 16, 12, 12]           6,416\n",
      "       BatchNorm2d-7           [-1, 16, 12, 12]              32\n",
      "            Conv2d-8             [-1, 16, 8, 8]           6,416\n",
      "         MaxPool2d-9             [-1, 16, 4, 4]               0\n",
      "          Dropout-10             [-1, 16, 4, 4]               0\n",
      "AdaptiveAvgPool2d-11             [-1, 16, 1, 1]               0\n",
      "           Linear-12                   [-1, 10]             170\n",
      "================================================================\n",
      "Total params: 19,898\n",
      "Trainable params: 19,898\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.34\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 0.42\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# summary\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00e420f9-2e03-44bb-bd09-c563f38350ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.280499\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.324991\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.220066\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.267609\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.088438\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.168880\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.038344\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.027229\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.074093\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.120426\n",
      "\n",
      "Test set: Average loss: 0.0630, Accuracy: 9784/10000 (97.84%)\n",
      "\n",
      "New best model saved with accuracy: 97.84%\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.135766\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.012893\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.116521\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.108540\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.047738\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.068655\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.052243\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.113237\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.034485\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.025855\n",
      "\n",
      "Test set: Average loss: 0.0458, Accuracy: 9854/10000 (98.54%)\n",
      "\n",
      "New best model saved with accuracy: 98.54%\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.021595\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.042746\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.002472\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.026421\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.088180\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.023200\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.070254\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.042439\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.055515\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.061231\n",
      "\n",
      "Test set: Average loss: 0.0745, Accuracy: 9749/10000 (97.49%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.006939\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.023160\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.006440\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.118935\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.009411\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.048719\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.009216\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.081574\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.162143\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.004233\n",
      "\n",
      "Test set: Average loss: 0.0293, Accuracy: 9899/10000 (98.99%)\n",
      "\n",
      "New best model saved with accuracy: 98.99%\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.018803\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.025262\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.026049\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.053983\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.023636\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.034436\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.058372\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.086629\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.059184\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.064492\n",
      "\n",
      "Test set: Average loss: 0.0281, Accuracy: 9920/10000 (99.20%)\n",
      "\n",
      "New best model saved with accuracy: 99.20%\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.056532\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.227435\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.018934\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.065745\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.014723\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.066198\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.006961\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.008751\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.015073\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.029123\n",
      "\n",
      "Test set: Average loss: 0.0283, Accuracy: 9917/10000 (99.17%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.032192\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.189602\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.029409\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.014856\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.041250\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.014410\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.008268\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.051719\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.009249\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.010933\n",
      "\n",
      "Test set: Average loss: 0.0273, Accuracy: 9915/10000 (99.15%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.026996\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.218423\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.020967\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.003561\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.014607\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.089599\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.044906\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.028920\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.074084\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.011466\n",
      "\n",
      "Test set: Average loss: 0.0390, Accuracy: 9872/10000 (98.72%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.024820\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.019997\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.041485\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.001867\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.061407\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.140950\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.048765\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.096543\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.122552\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.014030\n",
      "\n",
      "Test set: Average loss: 0.0299, Accuracy: 9908/10000 (99.08%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.003914\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.005571\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.001975\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.079775\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.002585\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.007275\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.003132\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.007050\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.006730\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.128139\n",
      "\n",
      "Test set: Average loss: 0.0179, Accuracy: 9939/10000 (99.39%)\n",
      "\n",
      "New best model saved with accuracy: 99.39%\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.014392\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.002233\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.014656\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.006696\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.004952\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.011727\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.004415\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.002702\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.001026\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.000443\n",
      "\n",
      "Test set: Average loss: 0.0210, Accuracy: 9930/10000 (99.30%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.044769\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.030341\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.001440\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.001177\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.133956\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.017090\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.070462\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.003527\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.000912\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.001652\n",
      "\n",
      "Test set: Average loss: 0.0217, Accuracy: 9925/10000 (99.25%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.050694\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.024142\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.003668\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.018350\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.011339\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.001862\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.011123\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.077134\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.050610\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.013393\n",
      "\n",
      "Test set: Average loss: 0.0180, Accuracy: 9938/10000 (99.38%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.000820\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.035291\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.068593\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.043051\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.001095\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.060979\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.103364\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.013472\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.000304\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.000823\n",
      "\n",
      "Test set: Average loss: 0.0193, Accuracy: 9934/10000 (99.34%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.011957\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 0.003754\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.013492\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.014387\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.009048\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.000947\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.004166\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 0.001147\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.026866\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.030253\n",
      "\n",
      "Test set: Average loss: 0.0161, Accuracy: 9943/10000 (99.43%)\n",
      "\n",
      "New best model saved with accuracy: 99.43%\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.015290\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 0.009509\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.009116\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 0.016906\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.002260\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.131830\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.037904\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 0.006307\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.007322\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 0.000315\n",
      "\n",
      "Test set: Average loss: 0.0148, Accuracy: 9954/10000 (99.54%)\n",
      "\n",
      "New best model saved with accuracy: 99.54%\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.049077\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 0.000565\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.000563\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 0.037403\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.000790\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.000577\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.100094\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 0.005749\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.058104\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 0.000582\n",
      "\n",
      "Test set: Average loss: 0.0147, Accuracy: 9947/10000 (99.47%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.011694\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 0.000501\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.001562\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 0.026876\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.040265\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.004400\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.003489\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 0.001211\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.021645\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 0.022320\n",
      "\n",
      "Test set: Average loss: 0.0197, Accuracy: 9939/10000 (99.39%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.000447\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 0.000842\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.015106\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 0.003880\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.000495\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.002954\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.023281\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 0.002732\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.006567\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 0.013952\n",
      "\n",
      "Test set: Average loss: 0.0247, Accuracy: 9921/10000 (99.21%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training Loop\n",
    "\n",
    "num_epochs = 19  # Back to 19 epochs\n",
    "\n",
    "best_accuracy = 0\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "\n",
    "    accuracy = test(model, device, test_loader)\n",
    "\n",
    "    # Step the scheduler based on validation accuracy\n",
    "\n",
    "    scheduler.step(accuracy) \n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "\n",
    "        best_accuracy = accuracy\n",
    "\n",
    "        torch.save(model.state_dict(), 'best_cnn_model.pth')\n",
    "\n",
    "        print(f'New best model saved with accuracy: {best_accuracy:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b685853d-f926-4f10-9c39-3dfed5316a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prade\\AppData\\Local\\Temp\\ipykernel_6512\\166518787.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_cnn_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0148, Accuracy: 9954/10000 (99.54%)\n",
      "\n",
      "Final Test Accuracy: 99.54%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and evaluate the best model\n",
    "\n",
    "model.load_state_dict(torch.load('best_cnn_model.pth'))\n",
    "\n",
    "final_accuracy = test(model, device, test_loader)\n",
    "\n",
    "print(f'Final Test Accuracy: {final_accuracy:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd1b4f48-3fb0-4fbb-93ba-cf95df56461f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prade\\AppData\\Local\\Temp\\ipykernel_6512\\183946868.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_cnn_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout1): Dropout(p=0.15, inplace=False)\n",
       "  (conv3): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (dropout2): Dropout(p=0.15, inplace=False)\n",
       "  (gap): AdaptiveAvgPool2d(output_size=1)\n",
       "  (fc1): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading on device\n",
    "# Load the model architecture and weights\n",
    "\n",
    "model = CNN()\n",
    "\n",
    "# Load the trained parameters (model weights)\n",
    "\n",
    "model.load_state_dict(torch.load('best_cnn_model.pth'))\n",
    "\n",
    "# If you're using a GPU, move the model to the correct device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Set the model to evaluation mode (important for dropout/batchnorm layers)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "923a1214-19d2-408f-a1c7-bcd5363df085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 99.84%\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to keep track of total correct predictions and total samples\n",
    "\n",
    "correct = 0\n",
    "\n",
    "total = 0\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "\n",
    "with torch.no_grad():  \n",
    "\n",
    "    for data, target in train_loader:\n",
    "\n",
    "        # Move data and target to the correct device\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        # Get predicted class by taking the index of the maximum value in the output\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "        # Update correct count\n",
    "\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        total += target.size(0)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "\n",
    "accuracy = 100. * correct / total\n",
    "\n",
    "print(f'Training Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd3261f7-13ce-44d7-b19e-2d9cce6005a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.54%\n"
     ]
    }
   ],
   "source": [
    "# Assuming test_loader is your DataLoader for the test data\n",
    "\n",
    "correct = 0\n",
    "\n",
    "total = 0\n",
    "\n",
    "# Disable gradient computation for inference (for better memory efficiency)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for data, target in test_loader:  # Use test_loader here for the test data\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        # Get predicted class by taking the index of the maximum value\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        # Update the correct count\n",
    "\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        total += target.size(0)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate and print accuracy for the test set\n",
    "\n",
    "accuracy = 100. * correct / total\n",
    "\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8982f2cb-eac4-44d9-9cfd-ec277d84e671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
